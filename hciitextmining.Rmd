---
title: "HCII Text-Mining"
author: "André Calero Valdez"
date: "10/16/2018"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    df_print: paged
    dev: png
    self_contained: true
    number_sections: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
knitr::asis_output("\U2500  \U2665  \U2666  \U2663")
lemmatization_url <- "https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-en.txt"

library(tidyverse)
library(tidytext)
library(pdftools)
library(stopwords)
library(purrr)
library(topicmodels)
library(viridis)
library(SnowballC)

alternate_palette <- function(palette){
  l <- length(palette)
  a <- palette[1:(l/2)]  # first half
  b <- palette[(floor(l/2)+1):l] # second half
  result <- c(a, b)[order(c(seq_along(a)*2 - 1, seq_along(b)*2))]
  result
}

## This function gets the text from a given file name
file2text <- function(filename) {
rawtext <- pdf_text(filename)
  text <- glue::glue_collapse(rawtext)
  text <- str_replace_all(text, "\\n", " ") %>% 
            str_to_lower() %>% 
            str_remove_all("([0-9]*)") %>% 
          str_replace_all("ﬀ", "ff") %>%  #ligatures
          str_replace_all("ﬁ", "fi") %>% 
          str_replace_all("ﬂ", "fl") %>%
          str_replace_all("ﬃ","ffi")%>%
          str_replace_all("ﬄ", "ffl") %>%
          str_replace_all("Ĳ", "IJ") %>%
          str_replace_all("ĳ", "ij") %>%
          str_replace_all("ﬆ", "st") %>%
          iconv(from="latin1", to="ASCII", "")

  text
}


## Function that extracts the paper name from AuthorYearTitle1:3
file2name <- function(filename) {
  filename <- filename %>%  iconv(from="latin1", to="ASCII", "")
  year <- str_extract(filename, "([0-9]+)")
  paper <- str_match_all(filename, "[0-9]+[ _]Chapter[ _](([A-Z]+[a-z]+)++)")
  author <- str_match(filename, "\\/([A-z- ]*)(?:[0-9]{4})")[1,2]
  paper <- unlist(as.vector(paper))
  # Concatenate Author, year, and first three Words
  paste0(author, year, paper[2])
}

# Debug Help (run file-getting from next chunk before)
#filename <- file_names[1]
#file2name(filename)

file2Year <- function(filename) {
  year <- str_extract(filename, "([0-9]+)")
  year <- as.numeric(year)
  year
}

const_first_year <- 2007
const_last_year <- 2017
const_cache_filename <- "temp/converted_texts.rds"
const_clean_start <- F

if(const_clean_start) {
  file.remove(const_cache_filename)
}

```

# Introduction 

## Loaded text-documents
```{r load_files}

# Ensure all data-frames are empty first
text_df <- data.frame()
text_df_cache <- data.frame()

#directory_name <- "/Volumes/Documents/#users/ACaleroValdez/hciiscrape"
#directory_name <- "data"
#file_names <- dir(path = directory_name, pattern = ".pdf", full.names = T, recursive = T)
file_names <- c()


# When a cache exists rewrite the file_names list with only missing files
if(file.exists(const_cache_filename)){
  text_df_cache <- readRDS(const_cache_filename)
  missing_files <- file_names [!  file_names %in% text_df_cache$filename]
  file_names <- missing_files
} 

text_df <- text_df_cache
file_count <- length(file_names)
if(file_count>0){
  step_size <- 2
  file_steps <- floor( file_count / step_size )
  
  for (i in 0:file_steps) {
    start_index <- i*step_size
    end_index <- (i+1)*step_size
    files_selected <- file_names[start_index:end_index]
    files_selected <- (files_selected)
    # Debugging if fail
    # print(files_selected)
    {
      text_list <- map_chr(files_selected, file2text)
      paper_list <- map_chr(files_selected, file2name)
      year_list <- map_dbl(files_selected, file2Year)
      new_text_df <- tibble(text = text_list, paper = paper_list,
                            year = year_list, filename = files_selected)
    }
    print(paste((i+1)*step_size, "files parsed. Start_i:", start_index, "End_i:", end_index ))
    
    # Merge Cache with new data
    text_df <- bind_rows(text_df, new_text_df)
    saveRDS(text_df, const_cache_filename, compress = "gzip")
  }
}
# cleanup
rm(text_list)
rm(paper_list)
rm(year_list)
rm(text_df_cache) 
rm(directory_name)
rm(file_names)
```


## Tokenization
After loading all files into a dataframe, which contains both a `paper` and a `year` field we can start the tokenization process.

```{r two-tokenizations, cache=T}

book_words_by_year <- text_df %>%
  unnest_tokens(word, text) %>% 
  mutate(stem = wordStem(word, language = "english")) %>% 
  group_by(year) %>% 
  count(paper, stem, sort = TRUE) %>%
  ungroup()  

book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  mutate(stem = wordStem(word, language = "english")) %>% 
  count(paper, stem, sort = TRUE) %>%
  ungroup()

# levels(factor(book_words$paper))

```
## Collect Stopwords

```{r stopwords}

# Get a list of stopwords
mystopwords <- tibble(word=stopwords("en", source = "stopwords-iso")) %>% 
  bind_rows(tibble(word= c("n", "s", "n", "t", "d", "e", 
                           "can", "based", "j", "r", "p", "x", "y",
                           "al", "et", "m", "pp", "fig", "also", "two", "different",
                           "use", "used", "using", "one", "e.g"))) 


# Create data-frames for papers individually and by year
book_words_cln <- book_words %>% rename(word = stem) %>% anti_join(mystopwords) 
book_words_by_year_cln <- book_words_by_year %>% rename(word = stem) %>% anti_join(mystopwords) 

```

## Analyze frequencies over time
```{r visualize-word-freq}

const_topn <- 7
book_words_by_year_cln %>% bind_tf_idf(word, paper, n) -> book_words_tfidf

book_words_by_year_cln %>%  # Use the by year data frame
  group_by(year, word) %>%  # and for all words and years
  summarize(n = sum(n)) -> all_words_by_year # count the word frequencies from all documents

all_words_by_year %>%     # get the top 10 words for each year
  bind_tf_idf(word, year, n) %>%  # create a tf-idf 
                                  #(where tf is word freq. and idf is based- on year)
  arrange(desc(tf)) -> top_words # sort top words

book_words_tfidf %>% arrange(desc(tf_idf))


#filter out top words
top_words %>% top_n(const_topn, tf) -> filter_top_words 

# get top words from all data to complete the data set
all_words_by_year %>% filter(word %in% filter_top_words$word) %>% 
  bind_tf_idf(word,year,n)-> final_word_list

# turn words into factors for completion 
final_word_list$word <- factor(final_word_list$word) 
# some words appear 0 times in some years, this requires to add 0s

level_count <- length(levels(final_word_list$word)) # generate a palette containg sufficient colors
palette <- viridis_pal(option = "D")(level_count)  

# create an alternating palette
p2 <- alternate_palette(palette)


# Save absolute figure ----
final_word_list %>%   complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%  # Reorder ?
  ggplot(aes(fill=word, y=n, x=year)) + geom_area(color="black") +
 # scale_fill_manual(values = palette) +
  labs (#title="Increasing importance of data for HCI research",
        title="Absolute Frequency over time",
        caption="",
        x="year",
        y="absolute term-frequency",
        fill="Most frequent terms") +
  scale_x_continuous(breaks = c(2007,2009,2011:2017)) +
  #scale_y_continuous(labels=scales::percent) +
  #theme(legend.position = "none") +
  NULL


ggsave("output/topabs.png", width=5, height=7*0.681)

```

## Relative output

```{r outputrelative}

# Save Relative figure -----
final_word_list %>%   complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%  # Reorder ?
  ggplot(aes(fill=word, y=tf, x=year)) + geom_area(color="black") +
 # scale_fill_manual(values = palette) +
  labs (subtitle="Increasing importance of data for HCI research",
        #subtitle="Relative Frequency over time",
        caption="",
        x="year",
        y="relative term-frequency",
        fill="Most frequent terms") +
  scale_x_continuous(breaks = c(2007,2009,2011:2017)) +
  scale_y_continuous(labels=scales::percent) +
  NULL


ggsave("output/toprel.png", width=7, height=7*0.681)



```


## Predict frequencies over time

```{r forcasting-by-loess}

library(forecast)
library(tidyquant)
library(imputeTS)

# get a complete dataset form the final_word_list
final_word_list %>%   complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%
  filter(word %in% c("gam","data", "inform", "design", "user", "web")) %>% 
  select(word, year, n) -> fc_data

# forcasting data plot as scatter plus loess_curve
fc_data %>% ggplot(aes(x = year, y = n, color = word)) +
    geom_point() +
    geom_smooth(method = "loess") + 
    labs(title = "Word Frequency over time", x = "", 
         y = "Word occurance") +
    facet_wrap(~ word, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq() +
    scale_x_continuous(breaks = c(2007,2009,2011,2013,2015,2017)) +
    theme_tq() +
    theme(legend.position="none")

ggsave("output/loess.png", width=7, height=7*0.681)

```

## Forcast the trends of top terms using random walks with drift

```{r foreast-plot}

# Constants -----
const_word_minimum_threshold <- 4000 


# Actual forcast plot ------------------


# from all cleaned words count how often they appear per year
book_words_by_year_cln %>% group_by(year, word) %>% 
  summarise(n = sum(n)) -> words_summary

# only select words that appear at leat <const_word_minimum_threshold> times
words_summary %>% filter(n > const_word_minimum_threshold) -> words_summary

#get a list of all words sorted in overall order
words_summary %>% 
  group_by(word) %>% 
  summarize(n = sum(n)) %>% 
  arrange(desc(n)) %>%  
  select(word) %>% as_vector() -> terms_list

# Run the predictions for all words
param_print_figures <- T # save images?
#generate empty data frame for succesive generation of forcasts
predictions <- data.frame() 

#debug 
#pred_word <- "design"

for(pred_word in head(terms_list, n=1000)){ 
  # from the subset of words, get only the word we are currently interested in
  # then select all its years and frequencies
   words_summary %>% filter(word == pred_word) %>% 
    select(year, n) -> word_freq 
  
  print(pred_word)
  # find missing years in the data 
  years <- const_first_year:const_last_year
  missing_years <- years[ !years %in% word_freq$year ]
  
  # Only do missing value imputation if at least 4 values are available
  available_years <- length(years) - length(missing_years)
  
  if(available_years > 3) {
    # kalman filter the missing year as create a time-series
    word_freq%>% 
      bind_rows(data.frame(year=missing_years, n=c(NA))) %>% 
      arrange(year) %>% ungroup() %>%  
      select(n) %>% as_vector %>% 
      imputeTS::na.kalman() %>%  
      ts(frequency = 1, start=const_first_year)  -> fc_ts 
    
    diff <- as.numeric(fc_ts[length(fc_ts)] - fc_ts[9])
    
    const_look_ahead = 2
    # use a random-walk forst to estimate the next 8 years
    fc_ts %>% 
      forecast::rwf(drift=T, h = const_look_ahead) %>% as.data.frame() %>% 
      rownames_to_column(var = "year") %>% 
      mutate(year=as.numeric(year)) %>% filter(year==const_last_year + const_look_ahead) %>% 
      mutate(value = `Point Forecast`, 
             ci_width = (`Hi 80` - `Lo 80`), 
             ci_lo = `Lo 80`, 
             ci_hi = `Hi 80`,
             change = diff) %>% 
      select(value, ci_width, ci_lo, ci_hi, change) %>% 
      mutate(word=pred_word) -> row_pred
    
    # Add prediction to the list of predictions
    predictions <- bind_rows(predictions, row_pred)
    
    if(param_print_figures){
      autoplot(fc_ts) + autolayer(
        forecast::rwf(fc_ts, drift = T, h = 3, level = 50),
        series = "Drift", PI = T, showgap = F) +
        ggtitle(paste("Forecasts for term", pred_word)) +
        xlab("Year") + ylab("Frequency") +
        expand_limits(y = c(0)) +
        scale_x_continuous(breaks = c(2007, 2011, 2015, 2019)) +
        guides(colour = guide_legend(title = "Forecast"))
      
      ggsave(paste0("output/",pred_word,".png"), width=3, height=3)
    }
  }
}


predictions %>% arrange(desc(value)) %>% head(40) %>% 
  mutate(word = factor(word, rev(unique(word)))) %>% 
  ggplot(aes(x=word, y= value, ymin = value - (ci_width / 2), ymax = value + (ci_width / 2))) + 
  geom_point() +
  geom_errorbar() +
  coord_flip() +
  NULL

predictions %>% arrange(desc(change)) %>% head(40) %>% 
  mutate(word = factor(word, rev(unique(word)))) %>% 
  ggplot(aes(x=word, y= change, ymin = value - (ci_width / 2), ymax = value + (ci_width / 2))) + 
  geom_point() +
  geom_errorbar() +
  coord_flip() +
  NULL

predictions %>% arrange(desc(value)) %>% head(40)
predictions %>% arrange(desc(change)) %>% head(100)
predictions %>% arrange(change) %>% head(100)
```




# Topic mining
Firt we prepare our data for latent-dirichlet allocation, by transforming the data from tidy to DTM (document-term-matrix) format.

```{r LDA-prep}

parallel::detectCores() -> cores_count
r_seed <- 77
words_used <- 100

book_words_cln %>%  arrange(desc(n)) %>% head(words_used) %>% cast_dtm(paper, word, n) -> book_dtm
book_words_by_year_cln %>%  arrange(desc(n)) %>% head(words_used) %>% cast_dtm(year, word, n) -> year_dtm

book_words_by_year_cln %>% filter(year==2017) %>% arrange(desc(n)) %>% head(words_used) %>% cast_dtm(paper, word, n) -> dtm_2017


dtm_choice <- dtm_2017
if(F){
library(ldatuning)
result <- FindTopicsNumber(
  dtm_choice,
  topics = seq(from = 40, to = 100, by = 10),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = r_seed, verbose=20),
  mc.cores = cores_count,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
}

k_parameter <- 70
lda_result <- LDA(dtm_choice, k = k_parameter, control = list(seed=r_seed))
book_lda <- LDA(book_dtm, k=k_parameter, control = list(seed=r_seed))
year_lda <- LDA(year_dtm, k=k_parameter, control = list(seed=r_seed))



#posterior(book_lda)
```
```{r}
library(text2vec)

tokens = book_words_cln %>% 
  tolower %>% 
  word_tokenizer
it = itoken(tokens, ids = movie_review$id[1:4000], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
```

## LDA 
Next we convert the lda_data back to tidy format for analysis with ggplot

```{r LDA2Tidy, fig.height=7}

book_tidy_lda <- tidy(book_lda, matrix="beta")

ap_top_terms <- book_tidy_lda %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  coord_flip()

ggsave("test.pdf", width=10, height=25)

beta_spread <- book_tidy_lda %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```

```{r next, EVAL=F}

#words per 
book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  count(paper, word, sort = TRUE) %>%
  ungroup()  

total_words <- book_words %>% 
  group_by(paper) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words %>% filter(paper == "Wetzlinger2014")

ggplot(book_words, aes(n/total, fill = paper)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~paper, ncol = 2, scales = "free_y")

freq_by_rank <- book_words %>% 
  group_by(paper) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

model <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset) # slope should be -1

summary(model)

```

```{r zipfs-law}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_abline(intercept = -0.9238, slope = -0.935, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.1, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() + labs(x="rank (log scale)", y="term frequency (log scale)", 
                         title="The observed word frequency follows a power distribution", 
                         subtitle="Word frequency over rank", 
                         caption="Dashed line refers to the linear model of parameters")


ggsave("output/zipf.png", width=7, height=7*0.681)

```


```{r further2}

book_words <- book_words %>%
  bind_tf_idf(word, paper, n)
book_words


book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))



book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(paper) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = paper)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~paper, ncol = 2, scales = "free") +
  coord_flip()



```

## Old Remove stopwords
List of removed stopwords was enriched manually by a set of natural speech stopwords, after manually scanning all utterences
```{r stop_word_removal}
en_manual_stopwords <- c(
  "ja", "–")

en_stopwords <- data_frame(word = stopwords::stopwords("en", source = "stopwords-iso")) %>% bind_rows(data_frame(word = en_manual_stopwords))

token_reduced_df <- token_df %>% anti_join(en_stopwords, by = "word")
```




# References
```{r}
citation("pdftools")
citation("tidytext")
citation("tidyverse")
citation("ggplot2")
citation("viridis")
citation("stopwords")
citation("topicmodels")
citation("rmarkdown")
citation("SnowballC")

citation("forecast")
citation("imputeTS")
citation("tidyquant")
```

