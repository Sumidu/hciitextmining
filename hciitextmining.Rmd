---
title: "HCII Text-Mining"
author: "André Calero Valdez"
date: "10/16/2018"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    df_print: paged
    dev: png
    self_contained: true
    number_sections: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
knitr::asis_output("\U2500  \U2665  \U2666  \U2663")
lemmatization_url <- "https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-en.txt"

library(tidyverse)
library(tidytext)
library(pdftools)
library(stopwords)
library(purrr)
library(topicmodels)
library(viridis)
library(SnowballC)

source("helpers.r")


# constants
const_first_year <- 2007
const_last_year <- 2017



# RESET ALL FILES
const_clean_start <- F



# DEBUG MODE (only 100 files)
debug_mode <- T


```




# Introduction 

## Loaded text-documents
```{r load_filenames}
# load filenames either from cache or from default directory on the server
file_names <- get_filenames(path="/Volumes/Documents/#users/ACaleroValdez/hciiscrape", clear_cache = F)
```


```{r load_files}
# cache for loaded files
const_cache_filename <- "temp/converted_texts.rds"
if(const_clean_start) {
  file.remove(const_cache_filename)
}


# When a cache exists rewrite the file_names list with only missing files
if(file.exists(const_cache_filename)){
  text_df_cache <- readRDS(const_cache_filename)
  if(debug_mode) {
    cachenum <- dim(text_df_cache)[1]
    file_names <- head(file_names, cachenum + 50)
  }
  #text_df_cache <- text_df_cache[!is.na(text_df$text),]
  missing_files <- file_names [!  file_names %in% text_df_cache$filename]
  file_names <- missing_files
  rm(missing_files)
  text_df <- text_df_cache
  rm(text_df_cache)
} else {
  # Ensure data frame exists
  text_df <- data.frame()

} 


# Stepping size for reading the files
step_size <- 10
#file_steps <- floor( file_count / step_size )

# How many files need reading in 
file_count <- length(file_names)



if(file_count>0){
  start_time <- Sys.time()
  pb <- txtProgressBar(min = 0, max = file_count, style = 2)
  for (i in 1:file_count) {
    setTxtProgressBar(pb, i)
    files_selected <- file_names[i]
    # Debugging if fail
    {
      text_list <- map_chr(files_selected, file2text2)
      paper_list <- map_chr(files_selected, file2name)
      year_list <- map_dbl(files_selected, file2Year)
      new_text_df <- tibble(text = text_list, paper = paper_list,
                            year = year_list, filename = files_selected)
    }

    # bind new row
    text_df <- bind_rows(text_df, new_text_df)
    stop_time <- Sys.time()
    diff_time <- difftime(stop_time, start_time, units = "secs")
    per_item_time = diff_time / i
    remaining_time <- (file_count - i ) * per_item_time
    
    rtime <- as.numeric(remaining_time)
    hours <- rtime %/% 3600
    mins <- (rtime - (hours*3600)) %/% 60
    secs <- round((rtime - (hours*3600) - (mins*60) ))
    
    
    # write cache every step_size message yes or no?
    
    if(i %% step_size == 0) {
      cat(paste("\n", i, "files of", file_count,"parsed. Cache updated. Expected runtime:",
                paste0(hours,"h:",mins,"m:",secs,"s"),"\n"))
      saveRDS(text_df, const_cache_filename, compress = "gzip")
    }
    
    # Merge Cache with new data
  }
  saveRDS(text_df, const_cache_filename, compress = "gzip")
  rm(text_list)
  rm(paper_list)
  rm(year_list)
  close(pb)
}
# cleanup
```


## Tokenization
After loading all files into a dataframe, which contains both a `paper` and a `year` field we can start the tokenization process.

```{r two-tokenizations, cache=T}

if(debug_mode){
  text_df <- sample_n(text_df, 400)
}


# test lemmatization
lemmatized_counts <- text_df %>% 
  unnest_tokens(word, text) %>% 
  mutate(stem = textstem::lemmatize_words(word)) %>% 
  count(year, paper, stem, sort = TRUE) %>% 
  ungroup()



# Tokens by Year
book_words_by_year <- text_df %>%
  unnest_tokens(word, text) %>% 
  mutate(stem = wordStem(word, language = "english")) %>% 
  group_by(year) %>% 
  count(paper, stem, sort = TRUE) %>%
  ungroup()  

book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  mutate(stem = wordStem(word, language = "english")) %>% 
  count(paper, stem, sort = TRUE) %>%
  ungroup()


```
## Collect Stopwords

```{r stopwords}

# Get a list of stopwords
mystopwords <- tibble(word=stopwords("en", source = "stopwords-iso")) %>% 
  bind_rows(tibble(word= c("n", "s", "n", "t", "d", "e", 
                           "can", "based", "j", "r", "p", "x", "y",
                           "al", "et", "m", "pp", "fig", "also", "two", "different",
                           "use", "used", "using", "one", "e.g"))) 


# Create data-frames for papers individually and by year
book_words_cln <- book_words %>% rename(word = stem) %>% anti_join(mystopwords) 
book_words_by_year_cln <- book_words_by_year %>% rename(word = stem) %>% anti_join(mystopwords) 
lemmatized_cln <- lemmatized_counts %>% rename(word = stem) %>% anti_join(mystopwords) 

```




## Analyze frequencies over time
```{r visualize-word-freq}

# how many top terms to include form each year.. larger samples require larger top n
const_topn <- 10
cleand_df <- lemmatized_cln 

cleand_df %>% bind_tf_idf(word, paper, n) -> df_tfidf

cleand_df %>%  # Use the by year data frame
  group_by(year, word) %>%  # and for all words and years
  summarize(n = sum(n)) -> all_words_by_year # count the word frequencies from all documents

all_words_by_year %>%     # get the top 10 words for each year
  bind_tf_idf(word, year, n) %>%  # create a tf-idf 
                                  #(where tf is word freq. and idf is based- on year)
  arrange(desc(tf)) -> top_words # sort top words


# generates a tf_idf table by 
df_tfidf %>% arrange(desc(tf_idf))


#filter out top words
top_words %>% top_n(const_topn, tf) -> filter_top_words 

# get top words from all data to complete the data set
all_words_by_year %>% filter(word %in% filter_top_words$word) %>% 
  bind_tf_idf(word,year,n)-> final_word_list

# turn words into factors for completion 
final_word_list$word <- factor(final_word_list$word) 
# some words appear 0 times in some years, this requires to add 0s

level_count <- length(levels(final_word_list$word)) # generate a palette containg sufficient colors
palette <- viridis_pal(option = "D")(level_count)  

# create an alternating palette
p2 <- alternate_palette(palette)


# Save absolute figure ----
final_word_list %>%   complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%  # Reorder ?
  ggplot(aes(fill=word, y=n, x=year)) + geom_area(color="black") +
 # scale_fill_manual(values = palette) +
  labs (#title="Increasing importance of data for HCI research",
        title="Absolute Frequency over time",
        caption="",
        x="year",
        y="absolute term-frequency",
        fill="Most frequent terms") +
  scale_x_continuous(breaks = c(2007,2009,2011:2017)) +
  #scale_y_continuous(labels=scales::percent) +
  #theme(legend.position = "none") +
  NULL


ggsave("output/topabs.png", width=5, height=7*0.681)




```

## Relative output

```{r outputrelative}

# Save Relative figure -----
final_word_list %>%   complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%  # Reorder ?
  ggplot(aes(fill=word, y=tf, x=year)) + geom_area(color="black") +
 # scale_fill_manual(values = palette) +
  labs (subtitle="Increasing importance of data for HCI research",
        #subtitle="Relative Frequency over time",
        caption="",
        x="year",
        y="relative term-frequency",
        fill="Most frequent terms") +
  scale_x_continuous(breaks = c(2007,2009,2011:2017)) +
  scale_y_continuous(labels=scales::percent) +
  NULL


ggsave("output/toprel.png", width=7, height=7*0.681)



```


## Predict frequencies over time

```{r forcasting-by-loess}

library(forecast)
library(tidyquant)
library(imputeTS)

# get a complete dataset form the final_word_list
final_word_list %>% group_by(word) %>% summarise(n=sum(n)) %>% top_n(10) %>% pull(word) -> trend_word_list

final_word_list %>% complete(word, year, fill = list(tf= 0.00, tf_idf=0)) %>% 
  arrange((tf)) %>% 
  mutate(word = factor(word, unique(word))) %>%
  filter(word %in% trend_word_list) %>% 
  select(word, year, n) -> fc_data

# forcasting data plot as scatter plus loess_curve
fc_data %>% ggplot(aes(x = year, y = n, color = word)) +
    geom_point() +
    geom_smooth(method = "loess") + 
    labs(title = "Word Frequency over time", x = "", 
         y = "Word occurance") +
    facet_wrap(~ word, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq() +
    scale_x_continuous(breaks = c(2007,2009,2011,2013,2015,2017)) +
    theme_tq() +
    theme(legend.position="none")

ggsave("output/loess.png", width=7, height=7*0.681)

```

## Forcast the trends of top terms using random walks with drift

```{r foreast-plot}

# Constants -----
const_count_predictions <- 100

# Actual forcast plot ------------------

cleand_df %>% 
# from all cleaned words count how often they appear per year
#book_words_by_year_cln %>% 
  group_by(year, word) %>% 
  summarise(n = sum(n)) -> words_summary

# only select words that appear at leat <const_word_minimum_threshold> times
#words_summary %>% filter(n > const_word_minimum_threshold) -> words_summary

#get a list of all words sorted in overall order
words_summary %>% 
  group_by(word) %>% 
  summarize(n = sum(n)) %>% 
  arrange(desc(n)) %>% 
  top_n(const_count_predictions,n) %>% 
  select(word) %>% as_vector() -> terms_list

# Run the predictions for all words
param_print_figures <- T # save images?
#generate empty data frame for succesive generation of forcasts
predictions <- data.frame() 

#debug 
#pred_word <- "design"

for(pred_word in terms_list){ 
  # from the subset of words, get only the word we are currently interested in
  # then select all its years and frequencies
   words_summary %>% filter(word == pred_word) %>% 
    select(year, n) -> word_freq 
  
  print(pred_word)
  # find missing years in the data 
  years <- const_first_year:const_last_year
  missing_years <- years[ !years %in% word_freq$year ]
  
  # Only do missing value imputation if at least 4 values are available
  available_years <- length(years) - length(missing_years)
  
  if(available_years > 3) {
    # kalman filter the missing year as create a time-series
    word_freq%>% 
      bind_rows(data.frame(year=missing_years, n=c(NA))) %>% 
      arrange(year) %>% ungroup() %>%  
      select(n) %>% as_vector %>% 
      imputeTS::na.kalman() %>%  
      ts(frequency = 1, start=const_first_year)  -> fc_ts 
    
    diff <- as.numeric(fc_ts[length(fc_ts)] - fc_ts[9])
    
    const_look_ahead = 2
    # use a random-walk forst to estimate the next 8 years
    fc_ts %>% 
      forecast::rwf(drift=T, h = const_look_ahead) %>% as.data.frame() %>% 
      rownames_to_column(var = "year") %>% 
      mutate(year=as.numeric(year)) %>% filter(year==const_last_year + const_look_ahead) %>% 
      mutate(value = `Point Forecast`, 
             ci_width = (`Hi 80` - `Lo 80`), 
             ci_lo = `Lo 80`, 
             ci_hi = `Hi 80`,
             change = diff) %>% 
      select(value, ci_width, ci_lo, ci_hi, change) %>% 
      mutate(word=pred_word) -> row_pred
    
    # Add prediction to the list of predictions
    predictions <- bind_rows(predictions, row_pred)
    
    if(param_print_figures){
      autoplot(fc_ts) + autolayer(
        forecast::rwf(fc_ts, drift = T, h = 3, level = 50),
        series = "Drift", PI = T, showgap = F) +
        ggtitle(paste("Forecasts for term", pred_word)) +
        xlab("Year") + ylab("Frequency") +
        expand_limits(y = c(0)) +
        scale_x_continuous(breaks = c(2007, 2011, 2015, 2019)) +
        guides(colour = guide_legend(title = "Forecast"))
      
      ggsave(paste0("output/",pred_word,".png"), width=3, height=3)
    }
  }
}


predictions %>% arrange(desc(value)) %>% head(const_count_predictions) %>% 
  mutate(word = factor(word, rev(unique(word)))) %>% 
  ggplot(aes(x=word, y= value, ymin = value - (ci_width / 2), ymax = value + (ci_width / 2))) + 
  geom_point() +
  geom_errorbar() +
  coord_flip() +
  NULL

predictions %>% arrange(desc(change)) %>% head(40) %>% 
  mutate(word = factor(word, rev(unique(word)))) %>% 
  ggplot(aes(x=word, y= change, ymin = value - (ci_width / 2), ymax = value + (ci_width / 2))) + 
  geom_point() +
  geom_errorbar() +
  coord_flip() +
  NULL

predictions %>% arrange(desc(value)) %>% head(40)
predictions %>% arrange(desc(change)) %>% head(100)
predictions %>% arrange(change) %>% head(100)
```




# Topic mining
First we prepare our data for latent-dirichlet allocation, by transforming the data from tidy to DTM (document-term-matrix) format.

```{r LDA-prep}

parallel::detectCores() -> cores_count
r_seed <- 77
words_used <- 100

cleand_df %>% cast_dtm(paper, word, n) -> lemma_dtm


# some other things here :D
#book_words_cln %>%  arrange(desc(n)) %>% head(words_used) %>% cast_dtm(paper, word, n) -> book_dtm
#book_words_by_year_cln %>%  arrange(desc(n)) %>% head(words_used) %>% cast_dtm(year, word, n) -> year_dtm
#book_words_by_year_cln %>% filter(year==2017) %>% arrange(desc(n)) %>% head(words_used) %>% cast_dtm(paper, word, n) -> dtm_2017


dtm_choice <- lemma_dtm
if(T){
  library(ldatuning)
  result <- FindTopicsNumber(
    dtm_choice,
    topics = seq(from = 10, to = 100, by = 10),
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
    method = "Gibbs",
    control = list(seed = r_seed, verbose=25),
    mc.cores = cores_count,
    verbose = TRUE
  )

  FindTopicsNumber_plot(result)
}



k_parameter <- 40
lda_result <- LDA(dtm_choice, k = k_parameter, 
                  control = list(seed = r_seed, verbose = 25))



#posterior(book_lda)
```







## LDA 
Next we convert the lda_data back to tidy format for analysis with ggplot

```{r LDA2Tidy, fig.height=7}
# Tidy lda data using beta
tidy_lda_beta <- tidy(lda_result, matrix="beta")

# group by topic and print top 15 words
ap_top_terms <- tidy_lda_beta %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  coord_flip()

ggsave("test.pdf", width=10, height=25)


```



```{r gamma}
# Tidy lda data using beta
tidy_lda_gamma <- tidy(lda_result, matrix="gamma")

# group by topic and print top 15 words
ap_top_terms <- tidy_lda_gamma %>%
  group_by(document) %>%
  top_n(5, gamma) %>%
  ungroup() %>%
  arrange(document, -gamma)

str_extract(ap_top_terms$document, ".*2")
unique(ap_top_terms$document)[6:10] ->example

ap_top_terms %>%
  filter(document %in% example) %>% 
  ggplot(aes(topic, gamma)) +
  geom_point() +
  facet_wrap(~ document)
```


```{r next, EVAL=F}

#words per 
book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  count(paper, word, sort = TRUE) %>%
  ungroup()  

total_words <- book_words %>% 
  group_by(paper) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words %>% filter(paper == "Wetzlinger2014")

ggplot(book_words, aes(n/total, fill = paper)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~paper, ncol = 2, scales = "free_y")

freq_by_rank <- book_words %>% 
  group_by(paper) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

model <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset) # slope should be -1

summary(model)

```

```{r zipfs-law}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_abline(intercept = -0.9238, slope = -0.935, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.1, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() + labs(x="rank (log scale)", y="term frequency (log scale)", 
                         title="The observed word frequency follows a power distribution", 
                         subtitle="Word frequency over rank", 
                         caption="Dashed line refers to the linear model of parameters")


ggsave("output/zipf.png", width=7, height=7*0.681)

```


```{r further2}

book_words <- book_words %>%
  bind_tf_idf(word, paper, n)
book_words


book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))



book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(paper) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = paper)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~paper, ncol = 2, scales = "free") +
  coord_flip()



```

## Old Remove stopwords
List of removed stopwords was enriched manually by a set of natural speech stopwords, after manually scanning all utterences
```{r stop_word_removal}
en_manual_stopwords <- c(
  "ja", "–")

en_stopwords <- data_frame(word = stopwords::stopwords("en", source = "stopwords-iso")) %>% bind_rows(data_frame(word = en_manual_stopwords))

token_reduced_df <- token_df %>% anti_join(en_stopwords, by = "word")
```



```{r}
library(text2vec)

text_df$id <- 1:dim(text_df)[1]
tokens <- text_df$text %>% 
  tolower() %>% 
  word_tokenizer()
it <- itoken(tokens, progressbar = TRUE)

v <- create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer <- vocab_vectorizer(v)
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)


doc_topic_distr <- 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
lda_model$get_top_words(n = 10, lambda = 0.5)
lda_model$plot(reorder.topics = T)
```



# References
```{r}
citation("pdftools")
citation("tidytext")
citation("tidyverse")
citation("ggplot2")
citation("viridis")
citation("stopwords")
citation("topicmodels")
citation("rmarkdown")
citation("SnowballC")

citation("forecast")
citation("imputeTS")
citation("tidyquant")
```

