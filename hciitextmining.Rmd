---
title: "HCII Text-Mining"
author: "André Calero Valdez"
date: "10/16/2018"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    df_print: paged
    dev: png
    self_contained: true
    number_sections: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
knitr::asis_output("\U2500  \U2665  \U2666  \U2663")

library(tidyverse)
library(tidytext)
library(pdftools)
library(stopwords)
library(purrr)
library(topicmodels)
library(viridis)
library(SnowballC)

alternate_palette <- function(palette){
  l <- length(palette)
  a <- palette[1:(l/2)]  # first half
  b <- palette[(floor(l/2)+1):l] # second half
  result <- c(a, b)[order(c(seq_along(a)*2 - 1, seq_along(b)*2))]
  result
}

```

# Introduction 

## Loaded text-documents
```{r load_files}

directory_name <- "data"
file_names <- dir(path = directory_name, pattern = ".pdf", 
                  full.names = T, recursive = T)



## This function gets the text from a given file name
file2text <- function(filename) {
rawtext <- pdf_text(filename)
  text <- glue::glue_collapse(rawtext)
  text <- str_replace_all(text, "\\n", " ") %>% 
            str_to_lower() %>% 
            str_remove_all("([0-9]*)")
  text
  }

file2name <- function(filename) {
  year <- str_extract(filename, "([0-9]+)")
  paper <- str_extract(filename, "[A-Z]+([a-z]+)")
  paste0(paper, year)
  
}

file2Year <- function(filename) {
  year <- str_extract(filename, "([0-9]+)")
  year <- as.numeric(year)
  year
}



text_list <- map_chr(file_names, file2text) 
paper_list <- map_chr(file_names, file2name)
year_list <- map_dbl(file_names, file2Year)
text_df <- tibble(text=text_list,  
                  paper=paper_list, year=year_list, filename=file_names) 
 
head(text_df, 3)
```


## Tokenization
After loading all files into a dataframe, which contains both a `paper` and a `year` field we can start the tokenization process.

```{r start-analysis}
    
tokenized_df <- text_df %>%
  unnest_tokens(word, text) 

book_words_by_year <- text_df %>%
  unnest_tokens(word, text) %>% 
  mutate(stem = wordStem(word, language = "english")) %>% 
  group_by(year) %>% 
  count(paper, word, sort = TRUE) %>%
  ungroup()  

book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  count(paper, word, sort = TRUE) %>%
  ungroup()

head(book_words)

```
## Collect Stopwords

```{r stopwords}

# Get a list of stopwords
mystopwords <- tibble(word=stopwords()) %>% 
  bind_rows(tibble(word= c("n", "s", "n", "t", "d", "e", 
                           "can", "based", "j", "r", "p", "x", 
                           "al", "et", "m", "pp", "fig", "also"))) 


# Create data-frames for papers individually and by year
book_words_cln <- book_words %>% anti_join(mystopwords) 
book_words_by_year_cln <- book_words_by_year %>% anti_join(mystopwords) 

```

```{r visualize-word-freq}

#book_words_by_year_cln %>% bind_tf_idf(word, year, n)

book_words_by_year_cln %>%  # Use the by year data frame
  group_by(year, word) %>%  # and for all words and years
  summarize(n = sum(n)) %>%   # count the word frequencies from all documents
  top_n(10, n) %>%            # get the top 10 words for each year
  bind_tf_idf(word, year, n) %>%  # create a tf-idf (where tf is word freq. and idf is based- on year)
  arrange(desc(tf_idf)) -> top_words # sort top words

top_words$word <- factor(top_words$word) # turn words into factors for completion 
# some words appear 0 times in some years, this requires to add 0s

level_count <- length(levels(top_words$word)) # generate a palette containg sufficient colors
palette <- viridis_pal(option = "D")(level_count)  

p2 <- alternate_palette(palette)

top_words %>%   complete(word, year, fill = list( tf= 0)) %>% 
  arrange(word) %>% 
  ggplot(aes(fill=word, y=tf, x=year)) + geom_area(position="stack", color="black") +
  scale_fill_manual(values = p2)




```

# Topic mining
Firt we prepare our data for latent-dirichlet allocation, by transforming the data from tidy to DTM (document-term-matrix) format.

```{r LDA-prep}

k_parameter <- 3
r_seed <- 123

book_words_cln %>%  cast_dtm(paper, word, n) -> book_dtm
book_words_by_year_cln %>% cast_dtm(year, word, n) -> year_dtm

book_lda <- LDA(book_dtm, k=k_parameter, control = list(seed=r_seed))
year_lda <- LDA(year_dtm, k=k_parameter, control = list(seed=r_seed))

```

## LDA 
Next we convert the lda_data back to tidy format for analysis with ggplot

```{r LDA2Tidy}

book_tidy_lda <- tidy(book_lda, matrix="beta")

ap_top_terms <- book_tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()



beta_spread <- book_tidy_lda %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```

```{r next, EVAL=F}

#words per 
book_words <- text_df %>%
  unnest_tokens(word, text) %>% 
  count(paper, word, sort = TRUE) %>%
  ungroup()  

total_words <- book_words %>% 
  group_by(paper) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words %>% filter(paper == "Wetzlinger2014")

ggplot(book_words, aes(n/total, fill = paper)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~paper, ncol = 2, scales = "free_y")

freq_by_rank <- book_words %>% 
  group_by(paper) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

model <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset) # slope should be -1

summary(model)

```

```{r zipfs-law}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = paper)) + 
  geom_abline(intercept = -0.9238, slope = -0.935, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.1, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() + labs(x="rank (log scale)", y="term frequency (log scale)", 
                         title="The observed word frequency follows a power distribution", 
                         subtitle="Word frequency over rank", 
                         caption="Dashed line refers to the linear model of parameters")


ggsave("zipf.png", width=7, height=7*0.681)

```


```{r further2}

book_words <- book_words %>%
  bind_tf_idf(word, paper, n)
book_words


book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))



book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(paper) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = paper)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~paper, ncol = 2, scales = "free") +
  coord_flip()



```

## Old Remove stopwords
List of removed stopwords was enriched manually by a set of natural speech stopwords, after manually scanning all utterences
```{r stop_word_removal}
en_manual_stopwords <- c(
  "ja", "–")

en_stopwords <- data_frame(word = stopwords::stopwords("en", source = "stopwords-iso")) %>% bind_rows(data_frame(word = en_manual_stopwords))

token_reduced_df <- token_df %>% anti_join(en_stopwords, by = "word")
```




# References
```{r}
citation("pdftools")
citation("tidytext")
citation("tidyverse")
citation("ggplot2")
citation("viridis")
citation("stopwords")
citation("topicmodels")
citation("rmarkdown")
citation("SnowballC")

```

